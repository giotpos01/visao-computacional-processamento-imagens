{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos generativos de texto\n",
    "\n",
    "Veremos passo-a-passo como usar um modelo generativo de texto do Hugginface. Veremos dois tipos de modelos:\n",
    "\n",
    "1. Modelos treinados apenas para completar textos\n",
    "2. Modelos refinados para conversa com o usuário\n",
    "\n",
    "É importante salientar que a biblioteca Hugginface possui os chamados *pipelines*, que permitem fazer o que veremos neste notebook de forma mais fácil. Mas pipelines abstraem totalmente os passos intermediários do processo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de geração de texto Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# Outros modelos possíveis:\n",
    "#'meta-llama/Llama-3.2-3B'\n",
    "#'meta-llama/Llama-3.1-8B'\n",
    "#'meta-llama/Llama-3.1-70B'\n",
    "#'meta-llama/Llama-3.1-405B'\n",
    "name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# O uso da biblioteca bitsandbytes permite a quantização dos pesos para reduzir uso de memória\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name, \n",
    "    torch_dtype=torch.bfloat16,  # Precisão para criar o modelo antes de carregar os pesos\n",
    "    device_map=\"auto\",           # Carrega os pesos na GPU até o limite de memória. Se precisar \n",
    "                                 # de mais memória, o restante do modelo é carregado na CPU.\n",
    "    quantization_config=quantization_config   # Estratégia de quantização\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    name, \n",
    "    padding_side=\"left\"         # Onde inserir o token de padding para criar batches de entrada\n",
    "    )\n",
    "\n",
    "# O modelo llama não foi treinado com um token de padding, mas ele é necessário para a inferência\n",
    "# utilizando batches. Podemos usar o token de final de sentença para representar o padding. Esse\n",
    "# token não é utilizado na atenção do modelo, então não deve afetar a geração de texto.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,     32,   1160,    315,   8146,     25,   2579,     11,   6437],\n",
       "        [128001, 128001, 128001, 128001, 128000,     16,     11,    220,     17]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenização de duas sentenças de texto\n",
    "texts = [\"A list of colors: red, blue\", \"1, 2\"]\n",
    "model_inputs = tokenizer(texts, padding=True, return_tensors=\"pt\").to(device)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>A list of colors: red, blue',\n",
       " '<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>1, 2']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decodificação dos tokens. Note que tokens extra foram adicionados para indicar o começo do texto,\n",
    "# o fim do texto e o padding\n",
    "tokenizer.batch_decode(model_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(128256, 2048)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A primeira camada do modelo é a camada de embedding. Ela transforma cada um dos 128k tokens \n",
    "# possíveis em um vetor possuindo 2048 valores. Usualmente, modelos treinados apenas no idioma \n",
    "# inglês possuem em torno de 40k tokens. Modelos multilinguagem precisam de mais tokens para \n",
    "# representar palavras em diferentes idiomas.\n",
    "model.model.embed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 2048])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = model.model.embed_tokens(model_inputs[\"input_ids\"])\n",
    "embeddings.shape\n",
    "# Saída possui tamanho bs x nro de tokens x d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 128256])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicação do modelo\n",
    "output = model(**model_inputs)\n",
    "output = output[\"logits\"]\n",
    "output.shape\n",
    "# Saída possui tamanho bs x nro de tokens x n_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A saída do modelo pode ser interpretada se seguinte forma. O vetor `output[0, i]` possui 128256 valores associados com os tokens de 0 a i-1 da sentença de entrada. Esse vetor representa os scores gerados pelo modelo para cada um dos 128256 tokens do vocabulário. Quanto maior o valor do score, maior a chance de que o respectivo token apareça após os tokens de 0 a i-1.\n",
    "\n",
    "A saída possui esse padrão pois é assim que o modelo é treinado. Os tokens de 0 a i-1 são usados para prever o token i, e a função loss é calculada para cada valor de i. Mas para a inferência precisamos apenas dos scores do último token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128256])\n"
     ]
    }
   ],
   "source": [
    "next_token_logits = output[:, -1]\n",
    "print(next_token_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0b65a41450>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAESCAYAAAAhVxT/AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAIu5JREFUeJzt3X10k+X9P/B32rRpKWk0ZSEtfSBMJg9FFGzdXFeKsAIysLA5xFmr7kzFtgL1IKLzh91XCbgdf+jY6qZbPY4V3Y4UivirVGmLfBXRlgoVvlRceW6/naskbWnTh1y/PzCxoUmbpyZ3kvfrnJxj7vtKcl2H7c3F577u65YJIQSIiEhSwvzdASIiGorhTEQkQQxnIiIJYjgTEUkQw5mISIIYzkREEsRwJiKSILm/O3A1s9mMixcvQqlUQiaT+bs7REQeE0Kgo6MDCQkJCAtzbk4suXC+ePEikpKS/N0NIiKvO3fuHBITE51qK7lwViqVAK4MIjY21s+9ISLynNFoRFJSkjXfnCG5cLaUMmJjYxnORBRUXCnV8oIgEZEEMZyJiCSI4UxEJEGSqzkTEUnRgFngcHM72jp6oFFGIV2nRnjY6C33ZTgTEY2gsrEFxXuOo8XQYz0Wr4rCxiXTsDA1flR+k2UNIqJhVDa2YNX2eptgBoBWQw9Wba9HZWPLqPyuS+Gs1+uRlpYGpVIJjUaDnJwcnDx5cki7EydOYOnSpVCpVFAqlfj+97+Ps2fPeq3TRES+MGAWKN5zHPYeF2U5VrznOAbM3n+glEvhXFtbi/z8fBw6dAhVVVXo7+9HdnY2urq6rG2+/PJLZGRkYMqUKaipqcFnn32Gp59+GlFRUV7vPBHRaDrc3D5kxjyYANBi6MHh5nav/7ZLNefKykqb96WlpdBoNKirq0NmZiYA4KmnnsLtt9+O559/3tpu0qRJXugqEZFvtXU4DmZ32rnCo5qzwWAAAKjVagBXNi3au3cvvve972HBggXQaDS45ZZbsGvXLoffYTKZYDQabV5ERFKgUTr3L35n27nC7XAWQqCoqAgZGRlITU0FALS1taGzsxObN2/GwoULsW/fPixbtgzLly9HbW2t3e/R6/VQqVTWFzc9IiKpSNepEa+KgqMFczJcWbWRrlN7/bdlQgi3Ktn5+fnYu3cvDh48aN1l6eLFi5gwYQJWrlyJsrIya9ulS5ciJiYGO3bsGPI9JpMJJpPJ+t6yQYjBYODeGkTkd5bVGgBsLgxaArvknlkjLqczGo1QqVQu5ZpbM+fCwkJUVFSgurraZvu7cePGQS6XY9q0aTbtp06d6nC1hkKhsG5yxM2OiEhqFqbGo+SeWdCqbEsXWlWUU8HsLpcuCAohUFhYiPLyctTU1ECn09mcj4yMRFpa2pDldU1NTUhJSfG8t0REfrAwNR4/nqZF2nNVaO/qw6acVKxIT5bOHYL5+fkoKyvD7t27oVQq0draCgBQqVSIjo4GAKxbtw4rVqxAZmYm5s6di8rKSuzZswc1NTVe7zwRka+Eh8lgWc6cPml0b90GXCxrlJSUwGAwICsrC/Hx8dbXm2++aW2zbNkyvPzyy3j++ecxY8YMvPrqq3jrrbeQkZHh9c4TEflSd+8AACAqInzUf8vlsoYzHnjgATzwwANudYiISIrMZgFTvxkAEO2DcObeGkRETujpH7D+d3Qkw5mISBIsJQ0AiJIznImIJKG770o4K+RhCBvli4EAw5mIyCk934SzL0oaAMOZiMgp3b2+uxgIMJyJiJxiKWswnImIJMQSzr5Y4wwwnImInGJZrcGaMxGRhPSwrEFEJD0saxARSZClrDGGZQ0iIungag0iIgniTShERBLky+1CAYYzEZFTWNYgIpIgazhH+iY2Gc5ERE7gOmciIglizZmISIK6uVqDiEh6uvu4ZSgRkeT09LLmTEQkOZf7+gEAUSxrEBFJB5+EQkQkQVxKR0QkMUIIrtYgIpKavgGBAbMAINF1znq9HmlpaVAqldBoNMjJycHJkycdtn/ooYcgk8mwdetWT/tJROQ3llkzINGyRm1tLfLz83Ho0CFUVVWhv78f2dnZ6OrqGtJ2165d+Pjjj5GQkOC1zhIR+YOl3hweJkNEuMwnvyl3pXFlZaXN+9LSUmg0GtTV1SEzM9N6/MKFCygoKMC7776LxYsXD/udJpMJJpPJ+t5oNLrSJSKiUdc9aI2zTOabcPao5mwwGAAAarXaesxsNiM3Nxfr1q3D9OnTR/wOvV4PlUplfSUlJXnSJSIir/P18wMBD8JZCIGioiJkZGQgNTXVenzLli2Qy+V49NFHnfqeDRs2wGAwWF/nzp1zt0tERKPC19uFAi6WNQYrKCjA0aNHcfDgQeuxuro6vPjii6ivr3d66q9QKKBQKNztBhHRqPP1rduAmzPnwsJCVFRUoLq6GomJidbjH3zwAdra2pCcnAy5XA65XI4zZ87gsccew8SJE73VZyIin/L1U1AAF2fOQggUFhaivLwcNTU10Ol0Nudzc3Mxf/58m2MLFixAbm4u7r//fs97S0TkB/6oObsUzvn5+SgrK8Pu3buhVCrR2toKAFCpVIiOjkZcXBzi4uJsPhMREQGtVovrr7/ee70mIvIh62oNH90dCLhY1igpKYHBYEBWVhbi4+OtrzfffHO0+kdE5HeWdc5jfBjOLpc1XHX69GmXP0NEJCUBtZSOiChU+Hq7UIDhTEQ0In+s1mA4ExGNoMfH24UCDGciohFZVmuw5kxEJCEsaxARSZCvn4ICMJyJiEbk6+cHAgxnIqIRseZMRCRBLGsQEUkQLwgSEUlQwOznTEQUSvzxJBSGMxHRCLjxERGRxJjNAj193PiIiEhSTP1m639ztQYRkURYShoAECVnOBMRScLl3n4AgEIehrAwmc9+l+FMRDQMf2wXCjCciYiG5Y+noAAMZyKiYfnj7kCA4UxENCx/rHEGGM5ERMPq6rlyQbC7bwAfffkfDJiFT36X4UxE5EBlYwueLD8GAGj+qgsrXzmEjC37UdnYMuq/zXAmIrKjsrEFq7bX41J3n83xVkMPVm2vH/WAZjgTEV1lwCxQvOc47BUwLMeK9xwf1RKHS+Gs1+uRlpYGpVIJjUaDnJwcnDx50nq+r68P69evx4wZMxATE4OEhATce++9uHjxotc7TkQ0Wg43t6PF0OPwvADQYujB4eb2UeuDS+FcW1uL/Px8HDp0CFVVVejv70d2dja6uroAAJcvX0Z9fT2efvpp1NfXY+fOnWhqasLSpUtHpfNERKOhrcNxMLvTzh1yVxpXVlbavC8tLYVGo0FdXR0yMzOhUqlQVVVl0+b3v/890tPTcfbsWSQnJ3veYyKiUaZRRnm1nTtcCuerGQwGAIBarR62jUwmwzXXXGP3vMlkgslksr43Go2edImIyGPpOjXiVVFoNfTYrTvLAGhVUUjXOc4+T7l9QVAIgaKiImRkZCA1NdVum56eHjzxxBO4++67ERsba7eNXq+HSqWyvpKSktztEhGRV4SHybBxyTS75yxbH21cMg3ho7gRkkwI4dblxvz8fOzduxcHDx5EYmLikPN9fX248847cfbsWdTU1DgMZ3sz56SkJBgMBoefISLyhcrGFhTuOIK+gW9jMl4VhY1LpmFharzT32M0GqFSqVzKNbfKGoWFhaioqMCBAwccBvPPf/5zNDc3Y//+/cN2RqFQQKFQuNMNIqJRtTA1HtrYEzj3dTfWzJ+MW3RxSNepR3XGbOFSOAshUFhYiPLyctTU1ECn0w1pYwnmL774AtXV1YiLi/NaZ4mIfM3wzU0oS2Ym4LvfGeuz33UpnPPz81FWVobdu3dDqVSitbUVAKBSqRAdHY3+/n787Gc/Q319Pd5++20MDAxY26jVakRGRnp/BEREo2TALGD8Zm+Na6IjfPrbLoVzSUkJACArK8vmeGlpKe677z6cP38eFRUVAIAbb7zRpk11dfWQzxERSZlx0K3bsVIO55GuHU6cOHHENkREgcKyr8ZYhRwR4b7d7YJ7axAROWCpN6t8PGsGGM5ERA5dutwLALhmDMOZiEgyLDNnhjMRkYRcuvxNOEf7fqUZw5mIyAFLOPt6pQbAcCYicuhSN2vORESSY7CWNRjORESSwQuCREQSdMm6zpkXBImIJIPrnImIJIh3CBIRSYwQ4tt1zpw5ExFJQ1fvAPrNVzZy400oREQSYak3R8rDEBXh+6hkOBMR2WFdRhcdAZls9B9LdTWGMxGRHQY/1psBhjMRkV2X/LhSA2A4ExHZZVmp4Y8bUACGMxGRXf7c9AhgOBMR2eXPTY8AhjMRkV3+3PQIYDgTEdllrTmPYc2ZiEgyLDVnrtYgIpKQS6w5ExFJT0DVnPV6PdLS0qBUKqHRaJCTk4OTJ0/atBFC4JlnnkFCQgKio6ORlZWFzz//3KudJiIabf588jbgYjjX1tYiPz8fhw4dQlVVFfr7+5GdnY2uri5rm+effx4vvPACtm3bhk8++QRarRY//vGP0dHR4fXOExF5y4BZ4KMv/4PdDRdQ8z//i+6+AQBAU1sHBr7Znc6XZEIIt3/13//+NzQaDWpra5GZmQkhBBISErBmzRqsX78eAGAymTB+/Hhs2bIFDz300IjfaTQaoVKpYDAYEBsb627XiIicVtnYguI9x9Fi6LF7Pl4VhY1LpmFharxb3+9OrnlUczYYDAAAtVoNAGhubkZrayuys7OtbRQKBebMmYMPP/zQ7neYTCYYjUabFxGRr1Q2tmDV9nqHwQwArYYerNpej8rGFp/1y+1wFkKgqKgIGRkZSE1NBQC0trYCAMaPH2/Tdvz48dZzV9Pr9VCpVNZXUlKSu10iInLJgFmgeM9xjFQ+sJwv3nPcZyUOt8O5oKAAR48exY4dO4acu3rvUyGEw/1QN2zYAIPBYH2dO3fO3S4REbnkcHP7sDPmwQSAFkMPDje3j26nviF350OFhYWoqKjAgQMHkJiYaD2u1WoBXJlBx8d/W5tpa2sbMpu2UCgUUCgU7nSDiMgjbR3OBbOnn3GHSzNnIQQKCgqwc+dO7N+/Hzqdzua8TqeDVqtFVVWV9Vhvby9qa2tx6623eqfHREReolFG+eQz7nBp5pyfn4+ysjLs3r0bSqXSWkdWqVSIjo6GTCbDmjVrsGnTJkyePBmTJ0/Gpk2bMGbMGNx9992jMgAiInel69SIV0Wh1dAzYt1ZBkCrikK6Tu2LrrkWziUlJQCArKwsm+OlpaW47777AACPP/44uru78cgjj+Drr7/GLbfcgn379kGpVHqlw0RE3hIeJsPGJdOwanv9sO0sV8w2LpmG8DDfPE/Qo3XOo4HrnInI1yobW1BQdgT9DlZi+GOds1sXBImIgsmC6VpERYSh0zSADYumIDVBBciArzpN0CivlDJ8NWO2YDgTUcj7+nIfOk1XbtfOu3UioiLC/dwj7kpHRITT/7myP1C8KkoSwQwwnImIcPY/lwEAKXFj/NyTbzGciSjkWWbOKeoYP/fkWwxnIgp51pnzOM6ciYgkwzJznhjHmTMRkWSc+WbmnKzmzJmISBI6evrwn64rT9rmBUEiIomwzJrjYiKhjPLPw1ztYTgTUUg7I8FldADvECSiEDNgFjjc3I62jh6Mi1Gg9os2AEBMpBwDZuHz27QdYTgTUcgY7kGuH5z6Chlb9nu0wZE3saxBRCFBqg9ydYThTERBT8oPcnWEZQ0iCkqDa8ttRpNbD3L9wXfjRreTw2A4E1HQGa627CxfPcjVEYYzEQUVS23Z06KErx7k6gjDmYiChrO15eH4+kGujvCCIBEFjcPN7R6VMvzxIFdHOHMmooBnufj3/zxcAqf18EGu3sRwJqKA5unFv6dunwpNrMJvD3J1hOFMRAFpwCywbf8p/N/3mtz6vKW2/ECGTjKBPBjDmYgCTmVjC56p+BytRpNbn5dSbdkRhjMRBRRvLJWTUm3ZEYYzEQUMT5fK3fuDFCxKjZdUbdkRl5fSHThwAEuWLEFCQgJkMhl27dplc76zsxMFBQVITExEdHQ0pk6dipKSEm/1l4hCmKdL5RalxuMH342TfDADboRzV1cXZs6ciW3bttk9v3btWlRWVmL79u04ceIE1q5di8LCQuzevdvjzhJRaHP3lmoZgHgJ3FjiCpfLGosWLcKiRYscnv/oo4+Ql5eHrKwsAMCDDz6IP/3pT/j0009xxx13DGlvMplgMn1b1Dcaja52iYhChDu3VAfCxT97vH6HYEZGBioqKnDhwgUIIVBdXY2mpiYsWLDAbnu9Xg+VSmV9JSUlebtLRBQk0nVqxKui4ErEalVRKLlnlqQv/tkjE0K4fdFTJpOhvLwcOTk51mO9vb341a9+hddffx1yuRxhYWF49dVXkZuba/c77M2ck5KSYDAYEBsb627XiChIjbRaY/W865Cui8NXnSbJ3FhiNBqhUqlcyjWvr9Z46aWXcOjQIVRUVCAlJQUHDhzAI488gvj4eMyfP39Ie4VCAYVC4e1uEFGQWpgaj8J51+Gl90/ZHI8PgOVxrvBqOHd3d+PJJ59EeXk5Fi9eDAC44YYb0NDQgN/97nd2w5mIyFWWp5T8aPI4/Gx2omRmyN7k1XDu6+tDX18fwsJsS9nh4eEwm83e/CkiCkGWDY7eOdYKAPjJDfG448YJfu7V6HA5nDs7O3Hq1Lf/nGhubkZDQwPUajWSk5MxZ84crFu3DtHR0UhJSUFtbS1ef/11vPDCC17tOBGFFnsbHP1uXxNU0RFBU8oYzOULgjU1NZg7d+6Q43l5eXjttdfQ2tqKDRs2YN++fWhvb0dKSgoefPBBrF27FjLZyP/kcKdwTkTBzdFFQEuiSH01hju55tFqjdHAcCaiwXr7zfi+/n20d/XaPW/ZXe7g+tskW3N2J9f4JBQikqzKxhZ8X/+ew2AGbJ+WHUy48RERSY47ezX7+2nZ3sZwJiJJcXevZn8/LdvbGM5E5FeW5XFtHT1o/ncXtr7/hUufl8rTsr2N4UxEfuPp8/8sAm1TI2cwnInIL7zxRJO4mEg8tyxV0svo3MVwJiKf6+0348nyRo+CWR0TgY82zEOkPDgXnQXnqIhIspxZHjcSGYBNy2YEbTADnDkTkRcMvqg3LkYByICvOk1D/vuT0+0uX/C7WrDtPucIw5mIPOKti3rOWDt/Mgpumxx0F//sYTgT0YgczYzdWfrmjjAZsG3lLNx+Q3DPlgdjOBPRsHw5M3Zk28qbQiqYAYYzUcgbrl7sjRqxJ0KlvmwPw5kohElhVjzYtWPkeOmuWWi/3BuUTzdxBcOZKER54yYQb5IB0C+/AT/63nf83RVJYDgThRBLCaPV0I3/2ntCMsEcyuULRxjORCFCaiUMi1BaHucKhjNRgBg8623v6oV6rAKasY5v+JDShT17OFseHsOZKABIddbryJp510H3nbEO/8II9Yt9zmA4E/mQs7c5S33W6whnw97DcCbykUCb/Tqyet51SNfFcTY8yhjORD7wztEWPFJW7+9ueISzYt9iOBONsneOXkTBjiP+7obTrh0Tgf+zZLrNxUbOin2P4Uw0iiobW/BIWWAEsyV29ctncHYsAQxnIg85usinjo7Ek+WN/u6e07QsW0iKy+F84MAB/Pa3v0VdXR1aWlpQXl6OnJwcmzYnTpzA+vXrUVtbC7PZjOnTp+Mf//gHkpOTvdVvIp+zF8Lvn/hf7Gq46NFTPfxl8IU9li2kx+Vw7urqwsyZM3H//ffjpz/96ZDzX375JTIyMvDLX/4SxcXFUKlUOHHiBKKiorzSYSJfuPqGj/OXurE7QEP4arywFxhkQgi3b6+XyWRDZs533XUXIiIi8Le//c2t7zQajVCpVDAYDIiNjXW3a0Q2XFlfHAiz4Z/NmoAfTv6O03cI8sKef7mTa16tOZvNZuzduxePP/44FixYgCNHjkCn02HDhg1DSh8WJpMJJpPJ+t5oNHqzSxTCLIFcdbxV8mHrLM56Q4dXw7mtrQ2dnZ3YvHkznn32WWzZsgWVlZVYvnw5qqurMWfOnCGf0ev1KC4u9mY3KMQNmAW27T+F0v9uxqXuPn93Z1gj3ebMWW/o8mpZ4+LFi5gwYQJWrlyJsrIya7ulS5ciJiYGO3bsGPId9mbOSUlJLGuQSwbPkv/x6Xl0mvr93aVhxcVE4rllqZwBhwi/lzXGjRsHuVyOadOm2RyfOnUqDh48aPczCoUCCoXCm92gEBNot0WrYyLw0YZ5iJSH+bsrJGFeDefIyEikpaXh5MmTNsebmpqQkpLizZ+iECLlZ9y5wlKM2LRsBoOZRuRyOHd2duLUqVPW983NzWhoaIBarUZycjLWrVuHFStWIDMzE3PnzkVlZSX27NmDmpoab/abglwwXszjTR7kCpdrzjU1NZg7d+6Q43l5eXjttdcAAH/961+h1+tx/vx5XH/99SguLsYdd9zh1PdzKV3oCuRAVsdEYNmNE3DblPG8mEdDuJNrHl0QHA0M59ATSKsr7IUwg5dG4vcLgkSA6zd8SHV1hTomAnfMTEDitWOgHquANpYhTL7DcCaX2QvfNmOP5G9zHqsIx4qbkxyWHliGIClhOJNDwbLRzzXREbj/hxP5hGcKKAxnsivQ1g5fzVIbnj9NyxkwBSSGc4gKlrXD9qydP5mzZAp4DOcQcHUQf3K6Ha99eFryKyNcxU2BKJgwnAPcSCsjArFGbA/XEVOoYThL3NWbvqvHKqx7+AZL8DpiWV3BujGFIoazjwXbpu+jgasriBjOXheoa4D9jasriGwxnO1wZXbLma59I9WIWS8mGh7D+SqBvr7XV64OX8u/DnibM5F3hHw4D54lN/+7K6DX946m1fOuQ7oujjNdIh8J6nAOlWVmo4lrh4n8I2jC+eolZ7z45pxroiOQd2uKdVbMWjCRNARFOLNOPDLuQ0wUWAI+nCsbW7Bqez0k9cQAH+Pdc0TBJ6DDecAsULzneNAH89WbvlvuEGTwEgWvgA7nw83tAVnK4BpgIhpJQIdzW4c0g5lrgInIUwEdzhpllE9+Z82866D7zlin7hbkTJeIvCGgwzldp0a8KmrUShtc40tE/hLQ4RweJsPGJdM8Xq3BZWZEJDUBHc4AsDA1HiX3zHK4zpnLzIgoEAV8OANXAvrH07RDNqXnxTciClRhrn7gwIEDWLJkCRISEiCTybBr1y6HbR966CHIZDJs3brVgy46JzxMhh98Nw7LZiXilz+ahGU3TcAPvhvHYCaigORyOHd1dWHmzJnYtm3bsO127dqFjz/+GAkJCW53jogoVLlc1li0aBEWLVo0bJsLFy6goKAA7777LhYvXux254iIQpXXa85msxm5ublYt24dpk+fPmJ7k8kEk8lkfW8wGAAARqPR210jIvILS54J4fy6Mq+H85YtWyCXy/Hoo4861V6v16O4uHjI8aSkJG93jYjIrzo6OqBSqZxq69Vwrqurw4svvoj6+nrIZM5diNuwYQOKioqs781mM9rb2xEXF+f0d1gYjUYkJSXh3LlziI2NdemzgSDYxwdwjMGCY7QlhEBHR4dL1+C8Gs4ffPAB2trakJycbD02MDCAxx57DFu3bsXp06eHfEahUEChUNgcu+aaazzqR2xsbND+DwII/vEBHGOw4Bi/5eyM2cKr4Zybm4v58+fbHFuwYAFyc3Nx//33e/OniIiCmsvh3NnZiVOnTlnfNzc3o6GhAWq1GsnJyYiLi7NpHxERAa1Wi+uvv97z3hIRhQiXw/nTTz/F3Llzre8t9eK8vDy89tprXuuYOxQKBTZu3DikTBIsgn18AMcYLDhGz8mEK2s7iIjIJ1y+Q5CIiEYfw5mISIIYzkREEsRwJiKSIIYzEZEEBU04//GPf4ROp0NUVBRmz56NDz74wN9dcpter0daWhqUSiU0Gg1ycnJw8uRJmzZCCDzzzDNISEhAdHQ0srKy8Pnnn/upx57R6/WQyWRYs2aN9VgwjO/ChQu45557EBcXhzFjxuDGG29EXV2d9Xygj7G/vx+//vWvodPpEB0djUmTJuE3v/kNzGaztU2gjXGk/eqdGY/JZEJhYSHGjRuHmJgYLF26FOfPn3e9MyIIvPHGGyIiIkK88sor4vjx42L16tUiJiZGnDlzxt9dc8uCBQtEaWmpaGxsFA0NDWLx4sUiOTlZdHZ2Wtts3rxZKJVK8dZbb4ljx46JFStWiPj4eGE0Gv3Yc9cdPnxYTJw4Udxwww1i9erV1uOBPr729naRkpIi7rvvPvHxxx+L5uZm8d5774lTp05Z2wT6GJ999lkRFxcn3n77bdHc3Cz++c9/irFjx4qtW7da2wTaGN955x3x1FNPibfeeksAEOXl5TbnnRnPww8/LCZMmCCqqqpEfX29mDt3rpg5c6bo7+93qS9BEc7p6eni4Ycftjk2ZcoU8cQTT/ipR97V1tYmAIja2lohhBBms1lotVqxefNma5uenh6hUqnEyy+/7K9uuqyjo0NMnjxZVFVViTlz5ljDORjGt379epGRkeHwfDCMcfHixeKBBx6wObZ8+XJxzz33CCECf4xXh7Mz47l06ZKIiIgQb7zxhrXNhQsXRFhYmKisrHTp9wO+rNHb24u6ujpkZ2fbHM/OzsaHH37op155l2WPa7VaDeDKLfOtra02Y1YoFJgzZ05AjTk/Px+LFy8esh9LMIyvoqICN998M+68805oNBrcdNNNeOWVV6zng2GMGRkZeP/999HU1AQA+Oyzz3Dw4EHcfvvtAIJjjIM5M566ujr09fXZtElISEBqaqrLYw74B7x+9dVXGBgYwPjx422Ojx8/Hq2trX7qlfcIIVBUVISMjAykpqYCgHVc9sZ85swZn/fRHW+88Qbq6+vxySefDDkXDOP717/+hZKSEhQVFeHJJ5/E4cOH8eijj0KhUODee+8NijGuX78eBoMBU6ZMQXh4OAYGBvDcc89h5cqVAILjz3EwZ8bT2tqKyMhIXHvttUPauJpHAR/OFlfv/SyEcHk/aCkqKCjA0aNHcfDgwSHnAnXM586dw+rVq7Fv3z5ERUU5bBeo4wOu7Et+8803Y9OmTQCAm266CZ9//jlKSkpw7733WtsF8hjffPNNbN++HWVlZZg+fToaGhqwZs0aJCQkIC8vz9oukMdojzvjcWfMAV/WGDduHMLDw4f8rdTW1jbkb7hAU1hYiIqKClRXVyMxMdF6XKvVAkDAjrmurg5tbW2YPXs25HI55HI5amtr8dJLL0Eul1vHEKjjA4D4+HhMmzbN5tjUqVNx9uxZAIH/ZwgA69atwxNPPIG77roLM2bMQG5uLtauXQu9Xg8gOMY4mDPj0Wq16O3txddff+2wjbMCPpwjIyMxe/ZsVFVV2RyvqqrCrbfe6qdeeUYIgYKCAuzcuRP79++HTqezOa/T6aDVam3G3Nvbi9ra2oAY87x583Ds2DE0NDRYXzfffDN+8YtfoKGhAZMmTQro8QHAD3/4wyHLH5uampCSkgIg8P8MAeDy5csIC7ONkPDwcOtSumAY42DOjGf27NmIiIiwadPS0oLGxkbXx+zWZUyJsSyl+8tf/iKOHz8u1qxZI2JiYsTp06f93TW3rFq1SqhUKlFTUyNaWlqsr8uXL1vbbN68WahUKrFz505x7NgxsXLlSkkvURrJ4NUaQgT++A4fPizkcrl47rnnxBdffCH+/ve/izFjxojt27db2wT6GPPy8sSECROsS+l27twpxo0bJx5//HFrm0AbY0dHhzhy5Ig4cuSIACBeeOEFceTIEeuyXGfG8/DDD4vExETx3nvvifr6enHbbbeF7lI6IYT4wx/+IFJSUkRkZKSYNWuWddlZIAJg91VaWmptYzabxcaNG4VWqxUKhUJkZmaKY8eO+a/THro6nINhfHv27BGpqalCoVCIKVOmiD//+c825wN9jEajUaxevVokJyeLqKgoMWnSJPHUU08Jk8lkbRNoY6yurrb7/728vDwhhHPj6e7uFgUFBUKtVovo6Gjxk5/8RJw9e9blvnA/ZyIiCQr4mjMRUTBiOBMRSRDDmYhIghjOREQSxHAmIpIghjMRkQQxnImIJIjhTEQkQQxnIiIJYjgTEUkQw5mISIL+P+SH5MhvlYLEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scores gerados para completar a sentença 0:\n",
    "probs = next_token_logits[0]\n",
    "probs = probs.detach().float().cpu().sort().values\n",
    "plt.plot(probs[-100:], \"-o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,     32,   1160,    315,   8146,     25,   2579,     11,   6437,\n",
       "             11,   6307,     11,  19087,     11,  14071,     11,  25977,     11,\n",
       "          18718,     11,  14198,     11,   3776,     11,   4251,     11,   5099,\n",
       "            627,     32],\n",
       "        [128001, 128001, 128001, 128001, 128000,     16,     11,    220,     17,\n",
       "             11,    220,     18,     11,    220,     19,     11,    220,     20,\n",
       "             11,    220,     21,     11,    220,     22,     11,    220,     23,\n",
       "             11,    220]], device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Geração de texto. O modelo simplesmente encontra os tokens com maior probabilidade \n",
    "# de completar os tokens de entrada\n",
    "# O resultado da geração consiste em \"nro tokens de entrada + max_new_tokens\".\n",
    "# Veja https://huggingface.co/docs/transformers/en/generation_strategies para mais detalhes\n",
    "# sobre as estratégias de geração\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens = 20)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A list of colors: red, blue, green, orange, yellow, purple, pink, brown, black, white, etc.\\nA',\n",
       " '1, 2, 3, 4, 5, 6, 7, 8, ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como o modelo gerou diversas palavras?\n",
    "\n",
    "Para cada novo token a ser gerado, a sequência inteira é passada pelo modelo para gerar atributos para o **último token da sequência de saída**. Inicialmente, a sequência possui 9 tokens e um novo token é gerado. Para gerar o próximo token, o novo token é concatenado ao final da sequência, criando uma sequência de 10 tokens. Essa sequência é então a nova entrada do modelo, e o próximo token é gerado. O processo continua até que a sequência possua tamanho `max_new_tokens`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo com refinamento de instrução\n",
    "\n",
    "Modelos *instruct* foram treinados para conversação. O modelo gera palavras que representem uma conversa natural com o usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# Outros modelos possíveis:\n",
    "#'meta-llama/Llama-3.2-3B-Instruct'\n",
    "#'meta-llama/Llama-3.1-8B-Instruct'\n",
    "#'meta-llama/Llama-3.1-70B-Instruct'\n",
    "#'meta-llama/Llama-3.1-405B-Instruct'\n",
    "name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name, \n",
    "    torch_dtype=torch.bfloat16,  \n",
    "    device_map=\"auto\",           \n",
    "    quantization_config=quantization_config\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    name\n",
    "    )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um modelo com instrução possui um template que indica como uma conversa deve ser formatada. Tipicamente, uma conversa possui os seguintes atores:\n",
    "\n",
    "1. system: Texto que é adicionado no começo da conversa ou a **todas as entradas do usuário**. Tipicamente, esse texto envolve instruções de como o modelo deve se 'comportar' durante toda a conversa. \n",
    "2. user: Representa o usuário\n",
    "3. assistant: Representa o modelo\n",
    "\n",
    "A conversa também pode ter artefatos como ferramentas (tools) e documentos. Esses artefatos não serão vistos neste notebook.\n",
    "\n",
    "Uma conversa é representada por um conjunto de mensagens, que são transformadas em uma única string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 08 Jun 2025\n",
      "\n",
      "You are a friendly chatbot who always responds in the style of a pirate<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How many cats does it take to change a light bulb?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de instrução de sistema e mensagem do usuário\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a pirate\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"How many cats does it take to change a light bulb?\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Conversão da conversa em uma string\n",
    "model_inputs = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True   # Adiciona o texto assistant ao final da string para guiar o modelo\n",
    "    )\n",
    "print(model_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string gerada acima possui um cabeçalho (system) com instruções para guiar o modelo, incluindo a data que o modelo foi treinado e a data atual, para que o modelo possa responder adequadamente perguntas que ultrapassem a data de treinamento.\n",
    "\n",
    "A lista de mensagens é transformada em uma string através de um template Jinja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   2318,  12044,    220,   2366,     20,    271,   2675,    527,\n",
       "            264,  11919,   6369,   6465,    889,   2744,  31680,    304,    279,\n",
       "           1742,    315,    264,  55066, 128009, 128006,    882, 128007,    271,\n",
       "           4438,   1690,  19987,   1587,    433,   1935,    311,   2349,    264,\n",
       "           3177,  46912,     30, 128009, 128006,  78191, 128007,    271]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criação dos tokens para entrada no modelo\n",
    "model_inputs = tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    add_generation_prompt=True, \n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True,\n",
    "    )\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
       "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
       "            220,   2318,  12044,    220,   2366,     20,    271,   2675,    527,\n",
       "            264,  11919,   6369,   6465,    889,   2744,  31680,    304,    279,\n",
       "           1742,    315,    264,  55066, 128009, 128006,    882, 128007,    271,\n",
       "           4438,   1690,  19987,   1587,    433,   1935,    311,   2349,    264,\n",
       "           3177,  46912,     30, 128009, 128006,  78191, 128007,    271,   9014,\n",
       "            637,     11,    430,    387,    264,    436,   3390,     11,  30276,\n",
       "             88,      0,    358,    387,   1781,    258,      6,    433,    387,\n",
       "            264,  28799,  14397,     11,    719,    358,    387,   1390,    258,\n",
       "              6,    311,   1440,    279,   4320,     13,   2650,   1690,  19987,\n",
       "            656,  20043,   1781,    387,   1205,    258,      6,    311,   2349,\n",
       "            264,   3177,  46912,     11]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs.to(device), do_sample=True, max_new_tokens=50)\n",
    "generated_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A saída do modelo inclui a entrada e os `max_new_tokens` gerados. Para obter apenas a saída, decodificamos apenas os novos tokens gerados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, that be a riddle, matey! I be thinkin' it be a clever trick, but I be wantin' to know the answer. How many cats do ye think be needin' to change a light bulb,\n"
     ]
    }
   ],
   "source": [
    "input_length = model_inputs[\"input_ids\"].shape[1]\n",
    "generated_ids[:, input_length:]\n",
    "print(tokenizer.decode(generated_ids[0, input_length:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vc2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
